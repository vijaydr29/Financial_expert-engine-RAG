{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:14:36.238 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:14:36.239 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:14:36.240 \n",
      "`st.cache` is deprecated and will be removed soon. Please use one of Streamlit's new\n",
      "caching commands, `st.cache_data` or `st.cache_resource`. More information\n",
      "[in our docs](https://docs.streamlit.io/develop/concepts/architecture/caching).\n",
      "\n",
      "**Note**: The behavior of `st.cache` was updated in Streamlit 1.36 to the new caching\n",
      "logic used by `st.cache_data` and `st.cache_resource`. This might lead to some problems\n",
      "or unexpected behavior in certain edge cases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache the function to load and process PDF documents\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_and_process_pdfs(pdf_folder_path):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:22:05.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.549 \n",
      "`st.cache` is deprecated and will be removed soon. Please use one of Streamlit's new\n",
      "caching commands, `st.cache_data` or `st.cache_resource`. More information\n",
      "[in our docs](https://docs.streamlit.io/develop/concepts/architecture/caching).\n",
      "\n",
      "**Note**: The behavior of `st.cache` was updated in Streamlit 1.36 to the new caching\n",
      "logic used by `st.cache_data` and `st.cache_resource`. This might lead to some problems\n",
      "or unexpected behavior in certain edge cases.\n",
      "\n",
      "2024-11-10 17:22:05.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.660 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-10 17:22:05.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "UnhashableParamError",
     "evalue": "Cannot hash argument 'splits' (of type `builtins.list`) in 'initialize_vectorstore'.\n\nTo address this, you can tell Streamlit not to hash this argument by adding a\nleading underscore to the argument's name in the function signature:\n\n```\n@st.cache_resource\ndef initialize_vectorstore(_splits, ...):\n    ...\n```\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:563\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     reduce_data \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reduce__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\copyreg.py:70\u001b[0m, in \u001b[0;36m_reduce_ex\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot pickle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m state \u001b[38;5;241m=\u001b[39m base(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'function' object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mUnhashableTypeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:430\u001b[0m, in \u001b[0;36m_make_value_key\u001b[1;34m(cache_type, func, func_args, func_kwargs, hash_funcs)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# we call update_hash twice here, first time for `arg_name`\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# without `hash_funcs`, and second time for `arg_value` with hash_funcs\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# to evaluate user defined `hash_funcs` only for computing `arg_value` hash.\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     \u001b[43mupdate_hash\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_hasher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnhashableTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:160\u001b[0m, in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, cache_type, hash_source, hash_funcs)\u001b[0m\n\u001b[0;32m    159\u001b[0m ch \u001b[38;5;241m=\u001b[39m _CacheFuncHasher(cache_type, hash_funcs)\n\u001b[1;32m--> 160\u001b[0m \u001b[43mch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:393\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdigest()\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:568\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdigest()\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:565\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableTypeError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n",
      "\u001b[1;31mUnhashableTypeError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnhashableParamError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m pdf_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fin_ed_docs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m splits \u001b[38;5;241m=\u001b[39m load_and_process_pdfs(pdf_folder_path)\n\u001b[1;32m---> 12\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:217\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m spinner(message, _cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_cached_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:231\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[1;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mget_function_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_key)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Generate the key for the cached value. This is based on the\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# arguments passed to the function.\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m value_key \u001b[38;5;241m=\u001b[39m \u001b[43m_make_value_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39msuppress(CacheKeyNotFoundError):\n\u001b[0;32m    240\u001b[0m     cached_result \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mread_result(value_key)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:438\u001b[0m, in \u001b[0;36m_make_value_key\u001b[1;34m(cache_type, func, func_args, func_kwargs, hash_funcs)\u001b[0m\n\u001b[0;32m    430\u001b[0m         update_hash(\n\u001b[0;32m    431\u001b[0m             arg_value,\n\u001b[0;32m    432\u001b[0m             hasher\u001b[38;5;241m=\u001b[39margs_hasher,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    435\u001b[0m             hash_source\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    436\u001b[0m         )\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnhashableTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 438\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnhashableParamError(cache_type, func, arg_name, arg_value, exc)\n\u001b[0;32m    440\u001b[0m value_key \u001b[38;5;241m=\u001b[39m args_hasher\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[0;32m    441\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache key: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, value_key)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:430\u001b[0m, in \u001b[0;36m_make_value_key\u001b[1;34m(cache_type, func, func_args, func_kwargs, hash_funcs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     update_hash(\n\u001b[0;32m    422\u001b[0m         arg_name,\n\u001b[0;32m    423\u001b[0m         hasher\u001b[38;5;241m=\u001b[39margs_hasher,\n\u001b[0;32m    424\u001b[0m         cache_type\u001b[38;5;241m=\u001b[39mcache_type,\n\u001b[0;32m    425\u001b[0m         hash_source\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    426\u001b[0m     )\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# we call update_hash twice here, first time for `arg_name`\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# without `hash_funcs`, and second time for `arg_value` with hash_funcs\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# to evaluate user defined `hash_funcs` only for computing `arg_value` hash.\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     \u001b[43mupdate_hash\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_hasher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnhashableTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableParamError(cache_type, func, arg_name, arg_value, exc)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:160\u001b[0m, in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, cache_type, hash_source, hash_funcs)\u001b[0m\n\u001b[0;32m    157\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mhash_source \u001b[38;5;241m=\u001b[39m hash_source\n\u001b[0;32m    159\u001b[0m ch \u001b[38;5;241m=\u001b[39m _CacheFuncHasher(cache_type, hash_funcs)\n\u001b[1;32m--> 160\u001b[0m \u001b[43mch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    321\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:393\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[1;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    321\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:568\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableTypeError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdigest()\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    321\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Users\\VDr2\\Downloads\\Generative AI\\Finexpert\\fin_env\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:565\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    563\u001b[0m     reduce_data \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__reduce__()\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableTypeError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(h, item)\n",
      "\u001b[1;31mUnhashableParamError\u001b[0m: Cannot hash argument 'splits' (of type `builtins.list`) in 'initialize_vectorstore'.\n\nTo address this, you can tell Streamlit not to hash this argument by adding a\nleading underscore to the argument's name in the function signature:\n\n```\n@st.cache_resource\ndef initialize_vectorstore(_splits, ...):\n    ...\n```\n            "
     ]
    }
   ],
   "source": [
    "# Import cache_resource from Streamlit\n",
    "from streamlit import cache_resource\n",
    "@st.cache(allow_output_mutation=True)\n",
    "# Cache the function to initialize the vector store with documents\n",
    "#@cache_resource\n",
    "def initialize_vectorstore(splits):\n",
    "    embeddings = OpenAIEmbeddings(api_key=st.secrets[\"openai\"][\"api_key\"])\n",
    "    return FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "pdf_folder_path = \"./fin_ed_docs\"\n",
    "splits = load_and_process_pdfs(pdf_folder_path)\n",
    "vectorstore = initialize_vectorstore(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a finance expert. You need to answer the question related to finance. \n",
    "Given below is the context and question of the user. Don't answer question outside the context provided.\n",
    "context = {context}\n",
    "question = {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, api_key=\"Your Key\")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
